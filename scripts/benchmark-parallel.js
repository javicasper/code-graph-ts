
import "dotenv/config";

const API_KEY = process.env.ZAI_API_KEY;
const BASE_URL = "https://api.z.ai/api/anthropic/v1/messages";

// ConfiguraciÃ³n de modelos con sus lÃ­mites especÃ­ficos
const MODEL_CONFIGS = [
    { name: "glm-4.5", limit: 10 },
    { name: "glm-4.7", limit: 3 },
    { name: "glm-4.5-air", limit: 5 },
    { name: "glm-4.6", limit: 3 },
    { name: "glm-4-plus", limit: 20 },
    { name: "glm-4.5-airx", limit: 5 },
    { name: "glm-4.7-flash", limit: 1 }
];

const SAMPLE_PROMPT = "Di 'OK' y nada mÃ¡s.";

async function testModel(modelName) {
    const start = Date.now();
    try {
        const response = await fetch(BASE_URL, {
            method: "POST",
            headers: {
                "x-api-key": API_KEY,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json",
            },
            body: JSON.stringify({
                model: modelName,
                max_tokens: 10,
                messages: [{ role: "user", content: SAMPLE_PROMPT }],
            }),
        });

        const latency = Date.now() - start;
        if (!response.ok) {
            return { ok: false, status: response.status, latency };
        }
        return { ok: true, latency };
    } catch (error) {
        return { ok: false, status: error.message, latency: Date.now() - start };
    }
}

async function benchmarkModelIndependent(config) {
    console.log(`\nï¿½ Probando ${config.name} con concurrencia ${config.limit}...`);

    // Vamos a lanzar 2 rondas de su lÃ­mite para ver estabilidad
    const totalRequests = config.limit * 2;
    const start = Date.now();

    // Lanzamos todas en paralelo (su lÃ­mite mÃ¡ximo)
    const promises = Array.from({ length: totalRequests }, () => testModel(config.name));
    const results = await Promise.all(promises);

    const totalTime = (Date.now() - start) / 1000;
    const successes = results.filter(r => r.ok).length;
    const avgLatency = Math.round(results.reduce((acc, r) => acc + r.latency, 0) / results.length);

    return {
        Modelo: config.name,
        LÃ­mite: config.limit,
        Ã‰xitos: `${successes}/${totalRequests}`,
        LatenciaMedia: `${avgLatency}ms`,
        "Desc/seg": (successes / totalTime).toFixed(2),
        Estado: successes === totalRequests ? "âœ… OK" : "âš ï¸ Inestable"
    };
}

async function run() {
    if (!API_KEY) {
        console.error("âŒ ZAI_API_KEY no encontrada.");
        return;
    }

    console.log("ðŸš€ Iniciando Benchmark Independiente por Modelo\n");
    const summary = [];

    for (const config of MODEL_CONFIGS) {
        const result = await benchmarkModelIndependent(config);
        summary.push(result);
        // PequeÃ±o respiro entre modelos
        await new Promise(r => setTimeout(r, 1000));
    }

    console.log("\nðŸ“Š Resumen de Rendimiento (Independiente):");
    console.table(summary);
}

run().catch(console.error);
